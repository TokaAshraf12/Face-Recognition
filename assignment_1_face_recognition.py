# -*- coding: utf-8 -*-
"""Assignment 1 - Face Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TERXcLJF7AUiA7_gMCbePwmIFRO_yGMO

# Importing Libraries
"""

import zipfile
import pandas as pd
from skimage import io
import numpy as np
from PIL import Image
import statistics as stat
from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from numpy.linalg import eig
import scipy
import os

"""# 1. Downloading The Dataset


"""

#!wget https://drive.google.com/file/d/1bIt96dpqtxrc7bHO1_uHeCSy0nqL3Ovj/view?usp=sharing
with zipfile.ZipFile('/content/archive.zip', 'r') as zip_ref:
    zip_ref.extractall('/content')

"""# 2. Generate the Data Matrix and the Label vector"""

dataset = [] 
y = []  
for i in range(40):
  for j in range(10):
    image = Image.open(f"/content/s{i + 1}/{j + 1}.pgm")
    image.show()
    image = io.imread(f"/content/s{i + 1}/{j + 1}.pgm", as_gray=True)
    vector = image.flatten()
    dataset.append(vector)
    y.append(i + 1)
D = pd.DataFrame(data=dataset)  # Data Matrix
print(D)

"""# 3. Split The Dataset Into Training and Test Sets"""

test_x=[]
test_y=[]
train_x=[]
train_y=[]
def split():
 for i in range(400):
   if i%2 ==0:
     test_x.append(dataset[i])
     test_y.append(y[i])
   else:
     train_x.append(dataset[i])
     train_y.append(y[i])  

split()

"""# Classification Using PCA"""

def compute_eigen(train_x):
  mean = np.mean(train_x,axis=0)
  Z = train_x - mean
  cov = np.cov(np.transpose(Z))
  eigenValues , eigenVectors = np.linalg.eigh(cov)

# sorted index
  idx = eigenValues.argsort()[::-1]   
  eigenValues = eigenValues[idx]
  eigenVectors = eigenVectors[:,idx]
  return eigenValues, eigenVectors

def projection_matrix(eigenValues,eigenVectors,alpha):
 eigenValuesSum = eigenValues.sum()
 sum = 0
 r = 0
 for i in eigenValues :
    sum = sum + i
    r = r + 1
    if((sum/eigenValuesSum)>=alpha):
      break
 p = eigenVectors[:,:r]
 return p

def pca(train_x,test_x,train_y,test_y):
  eigenValues,eigenVectors = compute_eigen(train_x)
  alpha =[0.8,0.85,0.9,0.95]
  k = [1, 3, 5, 7]
  x=[]
  for a in alpha:
    pca_accuracies =[]
    x=[]
    print("Alpha= ",a)
    for i in k :
     p = projection_matrix(eigenValues,eigenVectors,a)
     reduced_train_x = np.dot(train_x,p)
     reduced_test_x = np.dot(test_x,p)
     acc =knn_classification(reduced_train_x, train_y, reduced_test_x, test_y, i)
     pca_accuracies.append(acc)
     x.append(i)
     print("for k = "+ str(i) +" accuracy = "+str(acc))
   
    plot_accuracy(x,pca_accuracies, 'PCA')

"""# Classification Using LDA"""

def divide_train_data(trainx, slot):
  train_data=[]
  length = int(len(trainx) / slot)
  for i in range(length):
    train_data.append([])
  j=-1
  for i in range(len(trainx)):
    if(i % slot == 0):
      j+=1
    train_data[j].append(np.asarray(trainx)[i])
  train_data=np.asarray(train_data)
  return train_data

#Calculate the mean vector for every class Mu1, Mu2, ..., Mu40.
def get_mean_vectors(train_data):
   Mu=np.mean(train_data , axis=1)
   return Mu

def get_overall_sample_mean(train_data):
  Mu_overall=np.mean(train_data , axis=0)
  return Mu_overall

#calculate between class scatter matrix Sb
def calculate_Sb(Mu, Mu_overall, slot):
  Sb=np.zeros((len(Mu[0]),len(Mu[0])))  ## 10304*10304
  for i in range(len(Mu)):
    sub=(Mu[i]) - Mu_overall
    Sb =np.add(Sb , (slot * np.dot(sub.T , sub )) )

  return Sb

##center class matrix   200Ã—10304
def center_data(train_data,Mu):
  z=np.zeros(train_data.shape)
  for i in range(len(Mu)):
    z[i]= train_data[i] - Mu[i]
  return z

##class scatter matrices  S
def calculate_s_matrix(z_matrix):
  S=np.zeros((z_matrix.shape[2],z_matrix.shape[2])) ##10304*10304
  for i in range(z_matrix.shape[0]):
    S += np.dot(z_matrix[i].T , z_matrix[i])

  return S

## compute dominant eigen vector
# S^-1 * Sb
def compute_dominant_eigen(S , Sb):
  s_inverse=np.linalg.inv(np.asarray(S))
  product_res=np.dot(s_inverse , Sb) 
  eig_val, eig_vec = np.linalg.eigh(product_res)
    #get indexes of descendigly sorted eigen values array
  idx = eig_val.argsort()[::-1][:39]
    #get the 39 eigenvectors accordingly
  eig_vec = np.array(eig_vec[:,idx].real,dtype= np.float32)

  return eig_vec

##Project the training set, and test sets separately using the same 
##projection matrix U. You will have 39 dimensions in the new space.
def compute_train_test_projection(trainx,testx,eigen_vectors):  ##eigen vectors >> 10304*39
  training_set=np.dot(np.asarray(trainx) , eigen_vectors)
  test_set=np.dot(np.asarray(testx) , eigen_vectors)
  
  return training_set,test_set

def LDA(trainx,testx,slot):
  train_data=divide_train_data(trainx,slot)
  Mu=get_mean_vectors(train_data)
  Mu_overall_sample=get_overall_sample_mean(train_data)
  Sb=calculate_Sb(Mu,Mu_overall_sample,slot)

  z=center_data(train_data,Mu)
  S=calculate_s_matrix(z)
  eig_vec=compute_dominant_eigen(S,Sb)
  training_set,test_set=compute_train_test_projection(trainx,testx,np.real(eig_vec))
  return training_set,test_set

"""# Classifier Tunning"""

# Perform K-NN Classification
def knn_classification(train_x, train_y, test_x, test_y, k):
  knn = KNeighborsClassifier(n_neighbors=k)
  train_y = np.ravel(train_y)
  knn.fit(train_x, train_y)
  y_pred = knn.predict(test_x)
#  print_correction(y_pred, test_y)
  accuracy = accuracy_score(test_y, y_pred)
  return accuracy

def plot_accuracy(k, accuracy, t):
  plt.plot(k, accuracy, label=t)
  plt.xlabel('K')
  plt.ylabel('Accuracy')
  plt.show()

# Show Correction Of Prediction
def print_correction(prediction, test_y):
  print('=======================================================================')
  for j in range(len(prediction)):
      print("Picture number (" + str(j + 1) + ") is classified as (" + str(prediction[j]) + ") and is actually (" + str(test_y[j]) + ").")
      if((prediction[j]) != (test_y[j])):
        print("!! Classification error !!")

def lda_accuracy_plot_knn(train_x, test_x, slot):
  k = [1, 3, 5, 7]
  lda_accuracies = []
  lda_train_x, lda_test_x = LDA(train_x, test_x, slot)
  for i in k:
    lda_accuracy = knn_classification(lda_train_x, train_y, lda_test_x, test_y, i)
    lda_accuracies.append(lda_accuracy)
  plot_accuracy(k, lda_accuracies, 'LDA')
  return lda_accuracies

# For PCA Accuracy
pca(train_x,test_x,train_y,test_y)

# For LDA Accuracy
lda_accuracy_plot_knn(train_x, test_x, 5)

"""# Compare vs Non-Face Images"""

# !wget https://drive.google.com/file/d/1Zhtf20Ylwk4RKANcczpzFZFJ2mZxC7ay/view?usp=sharing
with zipfile.ZipFile('/content/non-face.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/non-face')

# Extracting the Non Faces Dataset
non_face_D = [] 
non_face_y = []  
path = '/content/non-face'
folders = os.listdir(path)
for folder in folders:
    files = os.listdir(path + '/' + folder)
    for file in files:
      image = Image.open(path + '/' + folder + '/' + file)
      gray_image = image.convert('L')
      # gray_image.show()
      image_array = np.array(gray_image)
      image_converted = np.resize(image_array, (10304))
      non_face_D.append(image_converted)
      non_face_y.append('non-face')

# Required Data
accuracies = []
no_non_faces = [50, 75, 100, 125, 150]
new_y = ['face' for i in range(200)]

for i in no_non_faces:
  combined_train_x = train_x + non_face_D[:i]
  train_y = new_y + non_face_y[:i]
  combined_test_x = test_x + non_face_D[i:i * 2]
  test_y = new_y + non_face_y[i:i * 2]
  accuracies.append(lda_accuracy_plot_knn(combined_train_x, combined_test_x, 5))
  print('Number of Non Faces is ' + str(i))
  print(accuracies)
plt.plot([i * 2 for i in no_non_faces], accuracies, label='Number of Non Faces vs Accuracy')
plt.xlabel('Number of Non Faces')
plt.ylabel('Accuracy')
plt.show()

# Computing The PCA For The Newly Combined Data
combined_train_x = train_x[:100] + non_face_D[:100]
train_y = new_y + non_face_y[:100]
combined_test_x = test_x[:100] + non_face_D[100:200]
test_y = new_y + non_face_y[100:200]
pca(combined_train_x, combined_test_x, train_y, test_y)

"""# Bonus"""

# Different Training and Test splits
test_x=[]
test_y=[]
train_x=[]
train_y=[]
def split73():
 for i in range(400):
   if i % 3 == 0 and i % 10 != 0:
     test_x.append(dataset[i])
     test_y.append(y[i])
   else:
     train_x.append(dataset[i])
     train_y.append(y[i])  
split73()

# Computing The PCA
pca(train_x,test_x,train_y,test_y)

# Computing The LDA
lda_accuracy_plot_knn(train_x, test_x, 7)

